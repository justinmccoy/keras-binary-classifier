{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotdog or NotHotdog?\n",
    "## Using PowerAI to Deploy an iOS Model for CoreML\n",
    "\n",
    "<strong><a href=https://developer.ibm.com/indexconf/sessions/#!?id=5466>Index 2018 </a></strong></br>\n",
    "\n",
    "Yes, really. <a href=\"https://twitter.com/mccoyjus\">Justin McCoy</a> and <a href=\"https://twitter.com/dokun24\">David Okun</a> will walk you through how to use PowerAI in the Cloud with Nimbix, to quickly train and deploy a Model that can tell you whether or not your lunchtime nutritional choice is the right one - all with the camera of the mobile phone in your pocket. All you need are some photos, descriptions of them, and you can be up and running with a model to stream video through in no time flat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you want to identify hotdogs, great, summer is just around the corner, and you can never be too careful of what you're eating. \n",
    "\n",
    "During this demo we will walk you through the steps, and technologies necessary to train a Deep Learning model using a Convolutional Neural Network, saving it into a format that can be loaded on an iOS device.\n",
    "\n",
    "I'm sure you've seen the eposide of Silicon Valley, \n",
    "\n",
    "\n",
    "\n",
    "\"edge computing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "1. Setup Environment \n",
    "2. Acquire Data\n",
    "3. Build Convolutional Neural Network\n",
    "4. Train Convolutional Neural Network\n",
    "5. Evaluate Model\n",
    "6. Make predictions\n",
    "7. Take a closer look at generated filters\n",
    "8. Save to CoreML\n",
    "9. Transfering Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Late in this Notebook the Keras Model is converted to Apple's coreML format; only Keras 2.0.4\n",
    "#  is supported as of creation of this notebook\n",
    "!sudo pip install keras==2.0.4\n",
    "!sudo pip install coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import json\n",
    "from matplotlib import pyplot\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "from tensorflow import Tensor\n",
    "\n",
    "from keras.engine import InputLayer\n",
    "import coremltools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ITERATION = 7\n",
    "\n",
    "DATA_PATH = 'hotdog_data'\n",
    "TRAINING_DATA_PATH = DATA_PATH + '/training_set'\n",
    "VALIDATION_DATA_PATH = DATA_PATH + '/validation_set'\n",
    "TEST_DATA_PATH = DATA_PATH + '/test_set'\n",
    "\n",
    "OUTPUT_PATH = 'output_' + time.strftime(\"%d-%m-%Y_\") + str(ITERATION)\n",
    "MODEL_JSON_PATH = OUTPUT_PATH + '/seefood_model.json'\n",
    "MODEL_WEIGHTS_PATH = OUTPUT_PATH + '/seefood_model_weights.h5'\n",
    "MODEL_EPOCH_PATH = OUTPUT_PATH + '/seefood_weights-{epoch:02d}-{val_acc}.hdf5'\n",
    "\n",
    "COREML_MODEL_PATH = OUTPUT_PATH + '/seefood_model.mlmodel'\n",
    "COREML_META_DESCRIPTION = 'SeeFood: Model to classify images as either hotdog or nothotdog'\n",
    "COREML_META_AUTHOR = 'Justin A. McCoy'\n",
    "COREML_META_INPUT_DESCRIPTION = 'Image of food that might be a hotdog'\n",
    "\n",
    "\n",
    "# input image dimensions\n",
    "IMG_ROWS, IMG_COLS = 128, 128\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "# Number of images gererated at each invocation of the ImageDataGenerator\n",
    "# Batchsize is an important value when training a CNN. too large a number can lead to out of memory errors, \n",
    "#  and lower accuracy. https://arxiv.org/abs/1606.02228 \n",
    "# When you have a high batch size in compairison to the number of training samples you make bigger jumps during\n",
    "# graident descent, this can lead you to the minimuma faster but it's possib\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "EPOCHS = 25\n",
    "\n",
    "HOTDOG = 0\n",
    "NOTHOTDOG = 1\n",
    "\n",
    "CLASSIFICATION = ['HOTDOG', 'NOTHOTDOG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir \"$OUTPUT_PATH\"\n",
    "print('Created path: {}'.format(OUTPUT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common methods used throughout notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_images(images, cols=1, title=\"\"):\n",
    "    \"\"\"Display a list of images in a single plot with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    fig = pyplot.figure()\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    for n, image in enumerate(images):\n",
    "        pyplot.subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n",
    "        pyplot.imshow(image)\n",
    "        pyplot.axis('off')\n",
    "    pyplot.show()\n",
    "    \n",
    "    \n",
    "def get_best_model():\n",
    "    return load_model(MODEL_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Acquire Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquire and classify training data for Hotdogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several places to aquire datasets that are classififed. for this example I've used Image.net to aquire images of hotdogs and food items that are not hotdogs\n",
    "\n",
    "www.image-net.org/\n",
    "\n",
    "ImageNet is a database of images organized according to the WordNet hierarchy, where each concept described by multiple words is grouped in a 'synonym set'. The synsets are thumbnails of images and contain links to images that have been classified.\n",
    "\n",
    "A major drawback of ImageNet revolves around where images are located; ImageNet doesn't keep a database of images but links to where images are, or were at one time. ImageNet doesn't save you from the task of cleaning and reviewing your data before jumping into the fun stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://ibm.box.com/s/ig5ao996ew9mqutfbgs0o2otophuwe1h\n",
    "tar -xzvf hotdog_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the training and validation data\n",
    "It's important to understand your data balance between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many images do we have to work with in our training and validation sets?\n",
    "\n",
    "hd_training_set_size = !ls -alR '$TRAINING_DATA_PATH'/hotdog | wc -l\n",
    "hd_training_set_size = hd_training_set_size[0]\n",
    "\n",
    "nhd_training_set_size = !ls -alR '$TRAINING_DATA_PATH'/not-hotdog | wc -l\n",
    "nhd_training_set_size = nhd_training_set_size[0]\n",
    "\n",
    "validation_set_size = !ls -alR '$VALIDATION_DATA_PATH'/hotdog | wc -l\n",
    "validation_set_size = validation_set_size[0]\n",
    "\n",
    "test_set_size = !ls -alR '$TEST_DATA_PATH'/ | wc -l\n",
    "test_set_size = test_set_size[0]\n",
    "\n",
    "print(\"Hotdog training examples:\\t{}\".format(hd_training_set_size))\n",
    "print(\"NotHotdog training examples:\\t{}\".format(nhd_training_set_size))\n",
    "print(\"Hotdog validation examples:\\t{}\".format(validation_set_size))\n",
    "print(\"Test examples:\\t{}\".format(test_set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation with Generators\n",
    "Using Keras's ImageDataGenerator we solve several problems with large datasets\n",
    "\n",
    "1. Eliminate the need to store our entire dataset in memory\n",
    "2. Supplement dataset with additional training images\n",
    "3. Improve model robustness by producing different color and oritntation profiles of images\n",
    "4. Mean-Normalization - Image pixel values are usually integers between the range of 1 and 255. Using such large numbers in models can cause overflow. To fix this the ImageDataGenerator provides a rescale function to scale the original pixel values by a sacaling factor, in our case this is 255. Leaving values between 0 and 1 for each pixel value.\n",
    "\n",
    "ere we’ve rescaled the image data so that each pixel lies in the interval [0, 1] instead of [0, 255]. It is always a good idea to normalize the input so that each dimension has approximately the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "    rotation_range=40,\n",
    "#    width_shift_range=0.2,\n",
    "#    height_shift_range=0.2,\n",
    "#    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "print(\"training_set\")\n",
    "training_set = train_datagen.flow_from_directory(TRAINING_DATA_PATH,\n",
    "    target_size = (IMG_ROWS, IMG_COLS),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode = 'binary')\n",
    "    \n",
    "print(\"validation_set\")\n",
    "validation_set = validation_datagen.flow_from_directory(VALIDATION_DATA_PATH,\n",
    "    target_size = (IMG_ROWS, IMG_COLS),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode = 'binary')\n",
    "\n",
    "print(\"test_set\")\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_set = test_datagen.flow_from_directory(TEST_DATA_PATH,\n",
    "                                            target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                            batch_size=488,\n",
    "                                            class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Keras's ImageDataGenerator we generate new images from transformations of our initial training set\n",
    "TRAINING_IMAGES = 20000\n",
    "VALIDATION_IMAGES = validation_set.samples # No augmentation of the validation images, just use what's there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at subset of sample generated images in the training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "    \n",
    "x,y = training_set.next()  # Generates BATCH_SIZE images on every invocation\n",
    "\n",
    "# Only interested in a subset of the images\n",
    "x = x[:9]\n",
    "y = y[:9]\n",
    "\n",
    "fig = pyplot.figure()\n",
    "title = fig.suptitle('training_set examples', fontsize=16)\n",
    "title.set_position([.5, 1.05])\n",
    "for n, (img, title) in enumerate(zip(x,y)):\n",
    "    a = fig.add_subplot(3, 3, n+1)\n",
    "    pyplot.imshow(img)\n",
    "    a.set_title(title)\n",
    "    pyplot.axis('off')\n",
    "#fig.set_dpi(300)\n",
    "pyplot.show()\n",
    "\n",
    "print(training_set.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A Convolutional Neural Network (CNN) \n",
    "\n",
    "Each filter in a CNN, learns different characteristic of an image.\n",
    "\n",
    "Keras allows us to specify the number of **filters** we want and the size of the filters. So, in our first layer, 32 is number of filters and (3, 3) is the size of the filter. We also need to specify the shape of the input which is (28, 28, 1), but we have to specify it only once.\n",
    "\n",
    "The second layer is the **Activation layer**. We have used ReLU (rectified linear unit) as our activation function. ReLU function is f(x) = max(0, x), where x is the input. It sets all negative values in the matrix ‘x’ to 0 and keeps all the other values constant. It is the most used activation function since it reduces training time and prevents the problem of vanishing gradients.\n",
    "\n",
    "The third layer is the **MaxPooling layer**. MaxPooling layer is used to down-sample the input to enable the model to make assumptions about the features so as to reduce over-fitting. It also reduces the number of parameters to learn, reducing the training time.\n",
    "\n",
    "After creating all the **convolutional layers**, we need to flatten them, so that they can act as an input to the Dense layers.\n",
    "\n",
    "**Dense layers** are keras’s alias for Fully connected layers. These layers give the ability to classify the features learned by the CNN.\n",
    "\n",
    "**Dropout** is the method used to reduce overfitting. It forces the model to learn multiple independent representations of the same data by randomly disabling neurons in the learning phase. In our model, dropout will randomnly disable 20% of the neurons.\n",
    "\n",
    "The second last layer is the Dense layer with 1 neuron. The neurons in this layer should be equal to the number of classes we want to predict as this is the output layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_learning_phase(1) # https://github.com/keras-team/keras/issues/2310\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(64, (3, 3), input_shape = (IMG_ROWS, IMG_COLS, IMG_CHANNELS), padding='same', activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "classifier.add(Dropout(0.4))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "classifier.add(Dropout(0.4))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(64))\n",
    "classifier.add(Activation('relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(1))\n",
    "classifier.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy loss calculates the error rate between the predicted value and the original value. The formula for calculating cross-entropy loss is given here. Because we have two classes we used binary_crossentropy.\n",
    "\n",
    "The Adam optimizer is an improvement over SGD(Stochastic Gradient Descent). The optimizer is responsible for updating the weights of the neurons via backpropagation. It calculates the derivative of the loss function with respect to each weight and subtracts it from the weight. This is how a neural network learns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save CNN as JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#serialize model to JSON\n",
    "classifier_json = classifier.to_json()\n",
    "with open(MODEL_JSON_PATH, \"w\") as json_file:\n",
    "    json_file.write(classifier_json)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics to monitor\n",
    "When training your convolutional neural network you're monitoring two things at each epoch for both the training and validation stages: **accuracy**, and **loss**.  As you monitor these metrics hopefully the accurracy goes up and the loss goes down. \n",
    "\n",
    "When training it's possible to **underfit** and **overfit**. Underfitting occurs when the accuracy on the training set is lower then the accuracy on the validation set; a poor performing model. Overfitting occurs when the training loss contines to go down, but the validation loss continues to rise.\n",
    "\n",
    "Finding the sweet spot between underfitting and overfitting is crucial to developing a model that generalizes well with unseen data.\n",
    "\n",
    "**Dropout Regularization** is one method that's used to reduce overfitting. Dropout randomly turns off neurons during training, reducing their weight further down the network. This encourages the network to find additional neurons building separate representations of the class within the network. Increasing the dropout rate between layers combats against overfitting; a good starting point is between 0.20 and 0.50.  \n",
    "\n",
    "#### *Finding the right balance is more of an art then science, and takes some time to experiment.*\n",
    "\n",
    "![XKCD.com/1838](https://imgs.xkcd.com/comics/machine_learning.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup callbacks to monitor metrics\n",
    "\n",
    "**Callbacks** are functions applied at given stages of training. Below is an example demostrating how to create a new callback, reporting the *validation loss* and *validation accuracy* at the end of each epoch.\n",
    "\n",
    "Keras comes with many callback functions, including ModelCheckpoint used to save the weights after every epoch as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('val_loss'))\n",
    "        print('-----------------------------------------------------------------------')\n",
    "        print('Epoch ' + str(epoch) + ' - Validation loss: ' + str(logs.get('val_loss')) + ' accuracy : ' + str(logs.get('val_acc')))\n",
    "        print('-----------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_all = ModelCheckpoint(filepath= MODEL_EPOCH_PATH, verbose=1, save_best_only=False)\n",
    "checkpoint_best = ModelCheckpoint(filepath= MODEL_WEIGHTS_PATH, verbose=1, save_best_only=True)\n",
    "loss_history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = classifier.fit_generator(training_set,\n",
    "    steps_per_epoch = TRAINING_IMAGES//BATCH_SIZE,\n",
    "    epochs = 50,\n",
    "    validation_data = validation_set,\n",
    "    validation_steps = VALIDATION_IMAGES//BATCH_SIZE,\n",
    "    callbacks = [loss_history, checkpoint_all, checkpoint_best])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter optimization is necessary to improve the models accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Accuracy and Loss over Epochs\n",
    "Plot the validation accuracy vs training accuracy, and the validation_loss vs training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pyplot.plot(history.epoch,history.history['val_acc'],label='validation accuracy')\n",
    "pyplot.plot(history.epoch,history.history['acc'],label='training accuracy')\n",
    "\n",
    "\n",
    "pyplot.legend(loc=0)\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.ylabel('accuracy')\n",
    "pyplot.grid(True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyplot.plot(history.epoch,history.history['val_loss'],label='validation loss')\n",
    "pyplot.plot(history.epoch,history.history['loss'],label='training loss')\n",
    "\n",
    "\n",
    "pyplot.legend(loc=0)\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.grid(True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best weights from the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The evaluate_generator method returns a tuple including the loss and accuracy of a model\n",
    "results = model.evaluate_generator(validation_set, 1)\n",
    "print(\"The model has a {}% accuracy, with a loss of {}.\".format(results[1]*100, results[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Confusion Matrix on with test data\n",
    "\n",
    "It's important to test your model with data that hasn't been used during training for validation. \n",
    "\n",
    "|               |   **HOTDOG**   |  **NOTHOTDOG** |\n",
    "| ------------- | -------------- | -------------- |\n",
    "| **HOTDOG**    | True Positive  | False Positive |\n",
    "| **NOTHOTDOG** | False Negative | True Negative  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the Confusion Matrix on our test data\n",
    "\n",
    "X_test, y_test = test_set.next()\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the best weights from the training \n",
    "model = get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_prediction(model, img):\n",
    "    \"\"\"Display a list of images in a single plot with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \"\"\"\n",
    "    test_image = image.img_to_array(img)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    return model.predict(test_image, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_positives=[]\n",
    "for file in os.listdir(TEST_DATA_PATH + '/nothotdog'):\n",
    "    img = image.load_img(TEST_DATA_PATH + '/nothotdog/' + file, target_size = (IMG_COLS, IMG_COLS))\n",
    "    if make_prediction(model, img) == HOTDOG:\n",
    "        false_positives.append(img)     \n",
    " \n",
    "show_images(false_positives, 3, title=\"False Positives\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_negatives=[]\n",
    "for file in os.listdir(TEST_DATA_PATH + '/hotdog'):\n",
    "    img = image.load_img(TEST_DATA_PATH + '/hotdog/' + file, target_size = (IMG_COLS, IMG_COLS))\n",
    "    if make_prediction(model, img) == NOTHOTDOG:\n",
    "        false_negatives.append(img)\n",
    "        \n",
    "\n",
    "show_images(false_negatives, 10, title=\"False Negatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images from the web to evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.dietsinreview.com/diet_column/wp-content/uploads/2010/07/joey-chestnut-nathans-famous-hot-dog-eating-contest.jpg -O hotdog.jpg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image = image.load_img('hotdog.jpg', target_size = (IMG_ROWS, IMG_COLS))\n",
    "pyplot.imshow(test_image)\n",
    "pyplot.show()\n",
    "\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "result = classifier.predict(test_image, verbose=1)\n",
    "\n",
    "print(CLASSIFICATION[int(result[0][0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget https://img.webmd.com/dtmcms/live/webmd/consumer_assets/site_images/dam/editorial/childrens-health/miscellaneous/how-to-change-teen-eating-habits/graphics/thumbnails/final/how-to-change-teen-eating-habits-375x321.jpg -O nothotdog.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image = image.load_img('nothotdog.jpg', target_size = (IMG_ROWS, IMG_COLS))\n",
    "pyplot.imshow(test_image)\n",
    "pyplot.show()\n",
    "\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "result = classifier.predict(test_image, verbose=1)\n",
    "\n",
    "print(CLASSIFICATION[int(result[0][0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Take a closer look at generated filters\n",
    "The CNN has several layers, including various filtering layers to identify important features of the image for the classification task. Let's look at the filters to see what areas of an image are identified as important for the hotdog, no hotdog classificatio\n",
    "Inspired by code here: https://github.com/mingruimingrui/Convolution-neural-networks-made-easy-with-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_layer_dict(model):\n",
    "    return dict([(layer.name, layer) for layer in model.layers if (layer.name.find('dense') > -1) | (layer.name.find('conv') > -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = get_layer_dict(model)\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deprocess_image(img):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    img -= img.mean()\n",
    "    img /= (img.std() + 1e-5)\n",
    "    img *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    img += 0.5\n",
    "    img = np.clip(img, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    img *= 255\n",
    "    img = np.clip(img, 0, 255).astype('uint8')\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_hidden_filter_layers(model, layer, num_plot=16):\n",
    "    _ = pyplot.suptitle(layer.name)\n",
    "\n",
    "    # we shall only plot out 16(default) as there are too many filters to visualize\n",
    "\n",
    "    layer_output = layer.output\n",
    "    output_shape = layer.output_shape\n",
    "    sub_plot_height = math.ceil(np.sqrt(num_plot))\n",
    "    nb_filters = output_shape[len(output_shape) - 1]\n",
    "\n",
    "    # here we need to conduct gradient acdent on each filter\n",
    "    counter = 0\n",
    "    for i in range(nb_filters):\n",
    "        if counter < num_plot:\n",
    "            # conv layers have different outputs than dense layers therefore different loss function sizes\n",
    "            if layer.name.find('conv') != -1:\n",
    "                loss = K.mean(layer_output[:,:,:,np.random.randint(nb_filters)])\n",
    "            else:\n",
    "                loss = K.mean(layer_output[:,np.random.randint(nb_filters)])\n",
    "\n",
    "            # randomise initial input_img and calc gradient\n",
    "            input_img = model.input#np.expand_dims(np.ones(X_shape[1:]), axis=0)\n",
    "            grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "            # normalize gradient\n",
    "            grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "            # this function returns the loss and grads given the input picture\n",
    "            iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "            # we start from a gray image with some noise\n",
    "            input_img_data = np.random.rand(1,IMG_ROWS, IMG_COLS, IMG_CHANNELS) * 0.1 + 0.5\n",
    "\n",
    "            # run gradient ascdent for 20 steps\n",
    "            for j in range(40):\n",
    "                loss_value, grads_value = iterate([input_img_data])\n",
    "                input_img_data += grads_value\n",
    "\n",
    "            # deprocess_image and plot if found\n",
    "            if loss_value > 0:\n",
    "                img = deprocess_image(input_img_data[0])\n",
    "                ax = pyplot.subplot(sub_plot_height, sub_plot_height, counter+1)\n",
    "                _ = pyplot.axis('off')\n",
    "                _ = ax.set_xticklabels([])\n",
    "                _ = ax.set_yticklabels([])\n",
    "                _ = ax.set_aspect('equal')\n",
    "                _ = pyplot.imshow(img.squeeze(), cmap='inferno')\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "    _ = pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer_name in layers:\n",
    "    plot_hidden_filter_layers(model, layers[layer_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualize image through each filter layer\n",
    "Run an image through each convolutional layer and display the different filters applied to the image. Each filter focuses on features within the image. This gives you a better understanding of what the CNN is looking for when making it's final decision; hotdog or nothotdog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_from_generator():\n",
    "    '''Generate some validation data and select a single sample image'''\n",
    "    X, y = validation_set.next()\n",
    "    img = X[0].reshape(-1,IMG_ROWS,IMG_COLS,IMG_CHANNELS)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conv_intermediate_layers(my_model):\n",
    "    '''Returns the names of each convolutional layer in the model'''\n",
    "    conv_layers = []\n",
    "    for layer in my_model.layers:\n",
    "        if 'conv' in layer.name:\n",
    "            # Build a new model with the input from the original \n",
    "            # model, but with the output of specific layer.\n",
    "            conv_layers.append(Model(inputs=my_model.input,\n",
    "                                     outputs=my_model.get_layer(layer.name).output))            \n",
    "    return conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_hidden_layers(my_model, img):\n",
    "    to_visual = my_model.predict(img)\n",
    "    to_visual = to_visual.reshape(to_visual.shape[1:])\n",
    "    _ = pyplot.figure()\n",
    "\n",
    "    sub_plot_height = math.ceil(np.sqrt(to_visual.shape[2]))\n",
    "    for i in range(to_visual.shape[2]):\n",
    "        ax = pyplot.subplot(sub_plot_height, sub_plot_height, i+1)\n",
    "        _ = pyplot.axis('off')\n",
    "        _ = ax.set_xticklabels([])\n",
    "        _ = ax.set_yticklabels([])\n",
    "        _ = ax.set_aspect('equal')\n",
    "        _ = pyplot.imshow(to_visual[:, :, i], cmap='inferno')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_models = get_conv_intermediate_layers(model)\n",
    "img = get_image_from_generator()\n",
    "\n",
    "_ = pyplot.imshow(img.reshape(img.shape[1:]))\n",
    "_ = pyplot.title('Hotdog')\n",
    "\n",
    "index = 0\n",
    "for my_model in conv_models:\n",
    "    index += 1\n",
    "    \n",
    "    plot_hidden_layers(my_model, img)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Convert Keras Model to Apple's coreML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the existing Keras model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the best weights from the training \n",
    "model = get_best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set model properties and save as coreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_labels = ['nothotdog']\n",
    "coreml_model = coremltools.converters.keras.convert(MODEL_WEIGHTS_PATH, input_names='image',image_input_names = 'image',class_labels = output_labels)   \n",
    "coreml_model.author = AUTHOR   \n",
    "coreml_model.short_description = COREML_META_DESCRIPTION \n",
    "coreml_model.input_description['image'] = COREML_META_INPUT_DESCRIPTION\n",
    "\n",
    "\n",
    "coreml_model.save(COREML_MODEL_PATH)\n",
    "\n",
    "print coreml_model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 9. Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the release of MobileNet https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html it's now possible to get much more accurate image classification on the edge of a network such as your smartphone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Import MobileNet and build classifier from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz\n",
    "!tar -xzvf mobilenet_v1_1.0_224_2017_06_14.tar.gz\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "\n",
    "# build the VGG16 network\n",
    "model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "bottleneck_features_validation = model.predict_generator(\n",
    "        validation_set, VALIDATION_IMAGES // BATCH_SIZE)\n",
    "\n",
    "np.save(open('bottleneck_features_validation.npy', 'w'), bottleneck_features_validation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
